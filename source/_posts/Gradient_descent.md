---
title: The mother of all optimization algorithms
date: 2020-08-1 21:58:47
tags: [Gradient Descent, Optimization, Machine Learning]
---

Everyone knows about Gradient Descent but nobody talks about it so often. Why is that behaviour? Well, running a ML algorithm in R or Python is really easy and by that what I mean is once you have prepared the Data, you just need to get the arguments correct and the data in the correct format. For example, XgBoost accepts data in XgB Matrix or One-hot encoding while Random Forest can accept Data as is. 

What next? You run the algorithm and you get the required results. Forget about Machine Learning Algorithms, even Linear and Logistic regression work on an optimising algorithm. That's why I say Gradient Descent : The mother of all optimization algorithms. Undoubtedly, this algorithm is at the backend of all the modelling techniques.

The earlier you get the basics right, the better you will be able to optimize your hyperparameters and modelling output.

In this article, I am going to talk about three things:
1. Theory of Gradient Descent
1. Practical implementation of Gradient Descent
1. What next if I understand and perfect the above two steps? Am I just giving gyaan or it can really help to be a better Data Scientist

Let's begin

**Gradient Descent**



